COT_prompt_template: >
  <|begin_of_text|><|start_header_id|>system<|end_header_id|> Answer the following question using the information given in the context below. Here is things to pay attention to:
    - First provide step-by-step reasoning on how to answer the question.
    - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context.
    - End your response with final answer in the form <ANSWER>: $answer, the answer should less than 60 words.
    You MUST begin your final answer with the tag "<ANSWER>: <|eot_id|>
  <|start_header_id|>user<|end_header_id|>
  Question: {question}\nContext: {context}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>

question_prompt_template: >
  <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a synthetic question-answer pair generator. Given a chunk of context about
  some topic(s), generate {num_questions} example questions a user could ask and would be answered
  using information from the chunk. For example, if the given context was a Wikipedia
  paragraph about the United States, an example question could be 'How many states are
  in the United States?
  The questions should be able to be answered in 100 words or less. Include only the
  questions in your response.<|eot_id|>
  <|start_header_id|>user<|end_header_id|>
  Context: {context}\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>

# question_prompt_template: >
#   You are a language model skilled in creating quiz questions.
#   You will be provided with a document,
#   read it and please generate question and answer pairs that are most likely be asked by a user of Llama language models
#   which includes LLama, Llama2, Meta Llama3, Code Llama, Meta Llama Guard 1,	Meta Llama Guard 2
#   Output only the questions related to Llama:
#   please make sure you follow those rules:
#   1. Generate {num_questions} question answer pairs, you can generate less answer if there is nothing related to model, training, fine-tuning and evaluation details of Llama language models, .
#   2. The questions can be answered based *solely* on the given passage.
#   3. Avoid asking questions with similar meaning.
#   4. Never use any abbreviation.
#   5. Include only the questions in your response.

data_dir: "/home/kaiwu/work/pytorch/docs"

xml_path: ""

chunk_size: 1000

questions_per_chunk: 5

num_distract_docs: 5 # number of distracting documents to add to each chunk

orcale_p: 0.8 # probability of related documents to be added to each chunk
